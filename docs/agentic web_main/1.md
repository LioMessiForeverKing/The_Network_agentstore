TheNetwork — Agentic Web v0

End-State System Specification (Target after 4-Day Build)

1. Purpose

This document defines what Agentic Web v0 should look like at the end of the current execution sprint.

By “Agentic Web v0” we mean:

A working Agent OS (Gaia) with:

routing

ranking (AgentRank v0)

passports

validation

learning loop

A single high-value enterprise workflow (Accounting v1) proving revenue potential.

A consumer app v0 that:

onboards students

builds digital DNA

routes to agents

demonstrates the identity/context layer.

This doc is the “end product spec”: what must exist, how it behaves, and what “done” means.

2. System Overview
2.1 High-Level Concept

TheNetwork is building the Agentic Web:

Users have digital DNA (preferences, behaviors, context).

There exists a registry of specialized agents (600+ experimental, a handful production).

A router (Gaia):

interprets user intent

selects the best agent(s)

validates output

learns from feedback

Agentic Web v0 consists of:

Gaia Router + AgentRank v0

Agent Registry + Passports (Humans + Agents)

Validator + Learning System

Enterprise Workflow: Accounting v1

Consumer App v0 (Student-facing)

AgentRank Dashboard + Usage Analytics

3. Architecture
3.1 Core Components

Gaia Router (gaia-router edge function)

Input: task_spec (task_type, raw_message, user_id, context)

Output: routing_decision + agent_response

Agents Registry (agents table)

Stores: slug, domain, description, invocation_type, invocation_config, status

Agent Passports (agent_capabilities table)

JSONB: capabilities, supported_task_types, constraints, metrics (success_rate, latency, trust), domain, input_format

User Passports (agent_passports table)

JSONB: digital DNA, preferences, history-based signals, social context

Validation + Learning System

Validator agent

Logs → agent_usage_logs

Metrics update functions

Enterprise Workflow: Accounting v1

A chain of specialized agents orchestrated via Gaia / workflows

Consumer App v0 (iOS)

Identity + context layer for 18–25yo users

Frontend for routing and agent interaction

AgentRank Dashboard (Web)

Observability and ranking view

4. Data Models (End-State)
4.1 Agents Table (agents)

Fields (already mostly exist, but must be normalized):

id (uuid)

name

slug (unique, kebab-case)

description (1–2 sentences, specific)

domain (enum: FINANCE, EDUCATION, WELLNESS, etc.)

invocation_type (enum: INTERNAL_FUNCTION, HTTP_ENDPOINT)

invocation_config (JSONB: endpoint, function name, parameters)

status (enum: ACTIVE, EXPERIMENTAL, DISABLED)

created_at, updated_at

End-state requirement:
All ~600 agents have valid slug, domain, description, invocation_type, and status (EXPERIMENTAL or ACTIVE).

4.2 Agent Capabilities / Agent Passport (agent_capabilities)

passport_data JSONB structure (end-state schema):

{
  "capabilities": {
    "supported_task_types": ["ACCOUNTING_CLASSIFICATION", "INVOICE_EXTRACTION"],
    "input_format": "standard", 
    "constraints": {
      "max_context_tokens": 8000,
      "allowed_domains": ["FINANCE"],
      "disallowed_actions": ["CALL_EXTERNAL_API_WITHOUT_ROUTER"]
    }
  },
  "metrics": {
    "success_rate_global": 0.93,
    "success_rate_recent": 0.96,
    "average_latency_ms": 1100,
    "total_uses": 523,
    "validation_events": 220
  },
  "learning": {
    "trend": "IMPROVING", 
    "last_30_days_success_delta": 0.04
  },
  "ranking_signals": {
    "agent_rank_score": 0.87,
    "domain_alignment_bonus": 0.05,
    "freshness_penalty": -0.02
  },
  "metadata": {
    "domain": "FINANCE",
    "tags": ["accounting", "invoice", "classification"]
  }
}


End-state requirement:

All ACTIVE agents used in production workflows have:

well-formed capabilities

metrics fields populated and updating

ranking_signals.agent_rank_score set by AgentRank v0.

4.3 User Passport (agent_passports)

passport_data JSONB structure (for users):

{
  "identity": {
    "user_id": "uuid",
    "name": "Ayen",
    "school": "Columbia",
    "year": 2027
  },
  "digital_dna": {
    "interests": {
      "music": 0.9,
      "startups": 0.95,
      "cs": 0.8,
      "f1": 0.7
    },
    "behavioral_signals": {
      "night_owl": true,
      "openness": 0.9
    }
  },
  "preferences": {
    "tone": "casual",
    "detail_level": "medium",
    "explain_steps": true
  },
  "app_usage": {
    "last_seen": "2025-12-03T...",
    "sessions": 42,
    "avg_session_length_min": 8.1
  }
}


End-state requirement:

At least 20–50 real student users with non-trivial passports.

Gaia router reads user passport and may adjust routing decision using their digital DNA (e.g., selecting more “tutor-like” agents vs “harsh-mentor” persona).

4.4 Usage Logs (agent_usage_logs)

Each routing + execution emits a log:

id

user_id (nullable for synthetic tests)

agent_id

task_type

full_context_json

input_json

output_json

success_flag (bool)

latency_ms

validation_score (float 0–1, from validator)

created_at

error_message (nullable)

End-state requirement:

All production paths are instrumented.

Dashboard uses these logs for metrics & AgentRank.

5. Gaia Router + AgentRank v0 (End-State Behavior)
5.1 Input

Gaia receives a task_spec:

{
  "user_id": "uuid-or-null",
  "task_type": "ACCOUNTING_CLASSIFICATION",
  "raw_message": "Here is an invoice PDF...",
  "context": {
    "file_refs": [...],
    "metadata": {...}
  }
}

5.2 Routing Steps (End-State)

Load user passport (if user_id given).

Filter candidate agents:

by supported_task_types

by status ∈ {ACTIVE, EXPERIMENTAL} depending on environment

Compute AgentRank v0 score for each candidate

AgentRank v0 formula (example spec):

Let:

SRg = global success rate

SRr = recent success rate (last N usages)

L = normalized latency score (lower latency = higher score)

V = average validator score (0–1)

D = domain alignment (1 if agent.domain == task.domain, else 0)

N = newness penalty (if total_uses < threshold)

Then:

agent_rank_score =
  0.30 * SRr +
  0.20 * SRg +
  0.20 * V +
  0.15 * L +
  0.10 * D -
  0.05 * N


Sort candidates by agent_rank_score

Select primary agent (top score) and optional backups.

Transform task to agent’s input_format

Invoke agent (edge function or HTTP endpoint).

Send agent output to Validator agent.

Store validation results + logs.

Return final response to caller.

End-state requirement:

All these steps are implemented, not just conceptual.

For a given task_type, multiple candidate agents exist, but ranking chooses one deterministically.

Router supports both:

production tasks (ACTIVE agents only)

synthetic tasks (EXPERIMENTAL agents for testing)

6. Validator + Learning System (End-State)
6.1 Validator Agent

Receives: task_spec, agent_output, agent_metadata.

Returns:

{
  "score": 0.0-1.0,
  "label": "PASS" | "FAIL" | "NEEDS_REVIEW",
  "rationale": "text explanation"
}

6.2 Learning Update

For each log entry:

Update:

success_rate_global

success_rate_recent

average_latency_ms

validation_events

trend (IMPROVING / STABLE / DECLINING)

Recompute agent_rank_score.

End-state requirement:

At least one full learning loop operational:

Agent is used → validator scores → metrics updated → ranking changes in future routes.

7. Enterprise Workflow: Accounting v1

This is the flagship workflow to show to an accounting firm and VCs.

7.1 Workflow Goal

Given:

an invoice PDF, or

a raw transaction line, or

a structured CSV row

produce:

normalized accounting entry JSON

classification into chart of accounts

validation & anomaly tagging

7.2 Workflow Stages

Input Ingestion

Accept file or structured JSON.

Convert into an intermediate invoice_representation:

{
  "source_type": "PDF" | "CSV" | "TEXT",
  "raw_content": "...",
  "metadata": {...}
}


Extractor Agent

task_type: INVOICE_EXTRACTION

Output JSON:

{
  "vendor": "string",
  "invoice_number": "string",
  "invoice_date": "YYYY-MM-DD",
  "due_date": "YYYY-MM-DD",
  "line_items": [
    {
      "description": "string",
      "quantity": 1,
      "unit_price": 100.0,
      "total": 100.0
    }
  ],
  "subtotal": 100.0,
  "tax": 8.88,
  "total": 108.88,
  "currency": "USD",
  "confidence": 0.0-1.0
}


Classification Agent

task_type: ACCOUNTING_CLASSIFICATION

Takes extractor output + client-specific chart of accounts.

Produces:

{
  "entries": [
    {
      "account_code": "6100",
      "account_name": "Office Supplies",
      "debit": 108.88,
      "credit": 0.0,
      "description": "Invoice #12345 - Vendor X",
      "tags": ["invoice", "office"]
    }
  ],
  "confidence": 0.0-1.0,
  "anomalies": []
}


Validator Agent

Scores output.

Flags anomalies (missing fields, mismatched totals, out-of-range classifications).

Final Output

Response JSON:

{
  "normalized_invoice": { ... },
  "accounting_entries": { ... },
  "validator": {
    "score": 0.94,
    "label": "PASS",
    "rationale": "All totals matched, account mapping consistent with prior invoices."
  },
  "metrics": {
    "latency_ms": 900,
    "agents_used": ["invoice-extractor-1", "accounting-classifier-1", "validator-1"]
  }
}


End-state requirement:

This workflow is:

executable end-to-end via an API call or internal tool

demo-ready for an accounting firm

stable for at least a small batch of test invoices

8. Consumer App v0 (Student-Facing)

This is the human context layer.

8.1 Core User Flows

Onboarding

Sign in (Google).

Basic profile: name, school, year.

Pre-seeded digital DNA via:

interests

simple survey or imported signals

Home / Star Map / Feed

User sees:

some representation of their interest graph or “star identity”.

Can tap “Send Pulse” / “Ask” button.

Send Pulse / Ask

Text input (and optionally attachments later).

Behind the scenes:

Build task_spec with user_id, raw_message, digital_dna.

Call Gaia router.

Get response from an agent.

Conversation View

User sees:

their message

agent response

(optional) “Handled by: [Agent Name]” and small indicator.

Profile View

Shows:

digital DNA summary (top interests)

maybe last few interactions

8.2 Technical Requirements

iOS build (TestFlight).

Integrated with Gaia router.

Real people (20–50) using it during test period.

Basic analytics: events for:

app_open

message_sent

agent_response_received

End-state requirement:

At least 20–50 real students can:

Log in

Send a query

Get an answer (from an actual agent via Gaia)

You have screenshots, logs, and feedback.

9. AgentRank Dashboard (Web)

Purpose: internal + investor view of the Agent OS brain.

9.1 Features

Agents Table

name, slug, domain

status (ACTIVE/EXPERIMENTAL)

success_rate_global

success_rate_recent

average_latency_ms

total_uses

agent_rank_score

Agent Detail View

metrics over time (success, latency)

last N validation events

example tasks routed

Learning Curves

simple time-series chart of:

success_rate_recent over time for select agents

Routing History

list of recent tasks:

task_type

chosen agent

fallback agent (if any)

validation score

End-state requirement:

Page at e.g. /admin/agents or /dashboard/agentrank.

Data read from real DB tables (agent_usage_logs, agent_capabilities, etc.).

10. Synthetic Testing Harness

To test routing + AgentRank across 600+ agents.

10.1 Capabilities

Generate or store test cases per task_type:

simple JSON list of input prompts per agent/domain.

Automatically:

call Gaia router with synthetic tasks

log routing decisions

optionally validate outputs for format

Output:

coverage metrics (how many agents have been hit)

error rates (failures / invalid outputs)

End-state requirement:

A script / edge function that can be run to exercise most experimental agents and confirm the router + ranking + invocation doesn’t break.

11. Non-Functional Requirements

Reliability

No blocking runtime errors on the main user paths:

consumer app query

accounting workflow run

dashboard view

Performance

Routing + inference total latency for simple tasks: target < 2 seconds typical.

Workflow latency: ideally < 3–4 seconds for a single invoice.

Observability

All production flows write to agent_usage_logs.

Basic alerting for systematic failure conditions (can be manual inspection at this stage).

12. Demo Script (End-State)

The live demo after the 4-day sprint should be able to follow this narrative:

Open AgentRank Dashboard

“This is our Agent OS brain. 600+ agents, ranked by AgentRank.”

Submit an accounting task

“Here’s an invoice. Watch the system route it.”

Show:

router selecting agents

workflow chain

final JSON output

validator PASS

Switch to Consumer App v0

“Now this is what a student sees.”

On TestFlight: send a question or “pulse”.

Show:

real-time agent handling

answer returned

Close with architecture

Briefly show:

passports

routing logic

logs

At the end, it should be obvious that:

You’re not just a “chat app.”

You’re not just “agents.”

You’ve built:

a routing and ranking brain,

an identity layer, and

a real enterprise-grade workflow.